{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Week 5 Day 2\n",
    "\n",
    "### AutoGen AgentChat - Going deeper..\n",
    "\n",
    "1. Multi-modal conversation\n",
    "2. Structured Outputs\n",
    "3. Using LangChain tools\n",
    "4. Teams\n",
    "\n",
    "...and a special surprise extra piece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "import requests\n",
    "from autogen_agentchat.messages import TextMessage, MultiModalMessage\n",
    "from autogen_core import Image as AGImage\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_core import CancellationToken\n",
    "from IPython.display import display, Markdown\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "load_dotenv(override=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A multi-modal conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# isn't just about text, but you can send pictures along with the text and have that be part of it.\n",
    "url = \"https://edwarddonner.com/wp-content/uploads/2024/10/from-software-engineer-to-AI-DS.jpeg\"\n",
    "\n",
    "pil_image = Image.open(BytesIO(requests.get(url).content))\n",
    "# create autogen image from that\n",
    "img = AGImage(pil_image)\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's a multi-modal message and it has content which has a list.\n",
    "# It has like a bit of text to describe the content of this image in detail.\n",
    "# The Message\n",
    "multi_modal_message = MultiModalMessage(content=[\"Describe the content of this image in detail\", img], source=\"User\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Model\n",
    "model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "\n",
    "# The Agent\n",
    "describer = AssistantAgent(\n",
    "    name=\"description_agent\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"You are good at describing images\",\n",
    ")\n",
    "\n",
    "response = await describer.on_messages([multi_modal_message], cancellation_token=CancellationToken())\n",
    "reply = response.chat_message.content\n",
    "display(Markdown(reply))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured Outputs!\n",
    "\n",
    "Autogen AgentChat makes it easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So this is a subclass of the Pydantic base model called image description.\n",
    "# And it's a class that I'm going to want to populate with the answer from the LLM.\n",
    "# And it's a class that I'm going to want to populate with the answer from the LLM.\n",
    "class ImageDescription(BaseModel):\n",
    "    scene: str = Field(description=\"Briefly, the overall scene of the image\")\n",
    "    message: str = Field(description=\"The point that the image is trying to convey\")\n",
    "    style: str = Field(description=\"The artistic style of the image\")\n",
    "    orientation: Literal[\"portrait\", \"landscape\", \"square\"] = Field(description=\"The orientation of the image\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "\n",
    "describer = AssistantAgent(\n",
    "    name=\"description_agent\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"You are good at describing images in detail\",\n",
    "    output_content_type=ImageDescription, # output content type equals and we pass in the pedantic object.\n",
    ")\n",
    "\n",
    "# The reply we expect to simply be a type of this object.\n",
    "# It's going to have replied with this object.\n",
    "# And remember, it feels as if the model is able to reply with a Python object.\n",
    "# And what's going on behind the scenes.\n",
    "# I know you know this.\n",
    "# It's all just JSON.\n",
    "# This is converted into some sort of a JSON spec, and the model returns JSON and the wrapper code then\n",
    "# populates this object from the JSON.\n",
    "response = await describer.on_messages([multi_modal_message], cancellation_token=CancellationToken())\n",
    "reply = response.chat_message.content\n",
    "# It is indeed an image description object.\n",
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So I'm using this thing called text wrap that prints something that's formatted, uh, to, to wrap\n",
    "# around at the end of a certain number of characters.\n",
    "# Summary:\n",
    "# The point of this was to show you that we can use structured outputs easily, and we can get back our\n",
    "# data.\n",
    "# That is according to this schema.\n",
    "import textwrap\n",
    "print(f\"Scene:\\n{textwrap.fill(reply.scene)}\\n\\n\")\n",
    "print(f\"Message:\\n{textwrap.fill(reply.message)}\\n\\n\")\n",
    "print(f\"Style:\\n{textwrap.fill(reply.style)}\\n\\n\")\n",
    "print(f\"Orientation:\\n{textwrap.fill(reply.orientation)}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LangChain tools from AutoGen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# We're going to use the tools that we worked with last week in long chain.\n",
    "# Autogen has a really easy way to wrap long chain tools so that you can call them directly from within\n",
    "# Autogen.\n",
    "\n",
    "# AutoGen's wrapper:\n",
    "# you can use that to wrap any long chain tool and it becomes an autogen tool.\n",
    "from autogen_ext.tools.langchain import LangChainToolAdapter\n",
    "\n",
    "# LangChain tools:\n",
    "\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain_community.agent_toolkits import FileManagementToolkit\n",
    "from langchain.agents import Tool\n",
    "\n",
    "\n",
    "prompt = \"\"\"Your task is to find a one-way non-stop flight from JFK to LHR in June 2025.\n",
    "First search online for promising deals.\n",
    "Next, write all the deals to a file called flights.md with full details.\n",
    "Finally, select the one you think is best and reply with a short summary.\n",
    "Reply with the selected flight only, and only after you have written the details to the file.\"\"\"\n",
    "\n",
    "# createing Goodle Serp api wrapper\n",
    "serper = GoogleSerperAPIWrapper()\n",
    "\n",
    "# Lang chain code to create a lang chain tool for searching the internet with a description based on this funciton\n",
    "langchain_serper =Tool(name=\"internet_search\", func=serper.run, description=\"useful for when you need to search the internet\")\n",
    "\n",
    "# And now I can call long chain tool adapter.\n",
    "# Create a new instance of that passing in the long chain tool.\n",
    "# And by passing that in this adapter adapts that long chain tool to become an autogen tool.\n",
    "# It's just sort of a wrapper around it.\n",
    "autogen_serper = LangChainToolAdapter(langchain_serper)\n",
    "\n",
    "autogen_tools = [autogen_serper]\n",
    "\n",
    "# And then I've also collected some long chain file tools by getting the file management toolkit giving\n",
    "# it a directory sandbox I've created an empty directory sandbox on the left.\n",
    "# And we call Get tools.\n",
    "# And we get a bunch of tools.\n",
    "# And for each of those, I'm going to add them to my autogen tools by appending a Lang chain tool adapter\n",
    "# and Autogen piece of Autogen code that adapts Lang Chains tool to be an Autogen tool.\n",
    "langchain_file_management_tools = FileManagementToolkit(root_dir=\"sandbox\").get_tools()\n",
    "for tool in langchain_file_management_tools:\n",
    "    autogen_tools.append(LangChainToolAdapter(tool))\n",
    "\n",
    "for tool in autogen_tools:\n",
    "    print(tool.name, tool.description)\n",
    "\n",
    "# Aim to call autogen agent\n",
    "model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "agent = AssistantAgent(name=\"searcher\", model_client=model_client, tools=autogen_tools, reflect_on_tool_use=True)\n",
    "message = TextMessage(content=prompt, source=\"user\")\n",
    "result = await agent.on_messages([message], cancellation_token=CancellationToken())\n",
    "for message in result.inner_messages:\n",
    "    print(message.content)\n",
    "display(Markdown(result.chat_message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we need to call the agent again to write the file\n",
    "# Now, one of the interesting things about Autogen is the way that it handles the interactions that agents\n",
    "# typically, uh, will then stop.\n",
    "# Um, particularly in this kind of mode of working with them.\n",
    "# And so what we now do is that I'm just going to send the next message of, okay, proceed, send a second\n",
    "# message to this agent in the same way.\n",
    "\n",
    "message = TextMessage(content=\"OK proceed\", source=\"user\")\n",
    "\n",
    "result = await agent.on_messages([message], cancellation_token=CancellationToken())\n",
    "for message in result.inner_messages:\n",
    "    print(message.content)\n",
    "display(Markdown(result.chat_message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Team interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uh, so you can create multiple assistants.\n",
    "\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.conditions import  TextMentionTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat\n",
    "\n",
    "from autogen_ext.tools.langchain import LangChainToolAdapter\n",
    "from langchain_community.utilities import GoogleSerperAPIWrapper\n",
    "from langchain.agents import Tool\n",
    "\n",
    "serper = GoogleSerperAPIWrapper()\n",
    "langchain_serper =Tool(name=\"internet_search\", func=serper.run, description=\"useful for when you need to search the internet\")\n",
    "autogen_serper = LangChainToolAdapter(langchain_serper)\n",
    "\n",
    "model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "prompt = \"\"\"Find a one-way non-stop flight from JFK to LHR in June 2025.\"\"\"\n",
    "\n",
    "# Agent that is doing the internet searches\n",
    "primary_agent = AssistantAgent(\n",
    "    \"primary\",\n",
    "    model_client=model_client,\n",
    "    tools=[autogen_serper],\n",
    "    system_message=\"You are a helpful AI research assistant who looks for promising deals on flights. Incorporate any feedback you receive.\",\n",
    ")\n",
    "\n",
    "# Agent that is doing the evaluation\n",
    "evaluation_agent = AssistantAgent(\n",
    "    \"evaluator\",\n",
    "    model_client=model_client,\n",
    "    system_message=\"Provide constructive feedback. Respond with 'APPROVE' when your feedback is addressed.\",\n",
    ")\n",
    "\n",
    "text_termination = TextMentionTermination(\"APPROVE\")\n",
    "\n",
    "# With thanks to Peter A for adding in the max_turns - otherwise this can get into a loop..\n",
    "# Um, and then we create a team, and there's various ways to create teams.\n",
    "# This is a bit like a crew and crew, but it's, uh, it's really, uh, somewhat simpler.\n",
    "# Really.\n",
    "# This is a round robin group chat, which means, like, one after the other, obviously.\n",
    "# Um, and, uh, that is the simplest way that you could have some kind of relationship between them.\n",
    "# And we pass in a list of agents to talk to each other, primary agent, and then an evaluation agent\n",
    "# and a termination condition that tells it.\n",
    "# When do you know that enough is enough?\n",
    "\n",
    "# Text mention termination.\n",
    "# The word approve.\n",
    "# So this is a little bit brittle.\n",
    "# I'm relying on the fact that the assistant, the evaluator agent, will reply the word approve.\n",
    "# Normally you would want something a little bit more profound than that.\n",
    "# You would probably want to reply, have structured outputs here and use that to test.\n",
    "# But this is perfectly good.\n",
    "# For now.\n",
    "team = RoundRobinGroupChat([primary_agent, evaluation_agent], termination_condition=text_termination, max_turns=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually I've been focusing a lot on the on messages.\n",
    "# The on messages, uh, thing that you call agents here.\n",
    "# You can also call Agent Run as well.\n",
    "# An agent can be called with run.\n",
    "# And then you just pass in this exactly the same thing.\n",
    "# You pass in the task, um, task equals and a prompt.\n",
    "# And what comes back is just going to be the final messages.\n",
    "# So that is another way of doing it as well.\n",
    "# Um, but this is what we will do now.\n",
    "\n",
    "result = await team.run(task=prompt)\n",
    "for message in result.messages:\n",
    "    print(f\"{message.source}:\\n{message.content}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drumroll..\n",
    "\n",
    "## Introducing MCP!\n",
    "\n",
    "Our first look at the Model Context Protocol from Anthropic -\n",
    "\n",
    "Autogen makes it easy to use MCP tools, just like LangChain tools."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/stop.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#ff7800;\">But wait - a not-so-small problem for Windows PC people</h2>\n",
    "            <span style=\"color:#ff7800;\">I have unpleasant news. There's a problem running MCP Servers on Windows PCs; Mac and Linux is fine. This is a known issue as of May 4th, 2025. I asked o3 with Deep Research to try to find workarounds; it <a href=\"https://chatgpt.com/share/6817bbc3-3d0c-8012-9b51-631842470628\">confirmed the issue</a> and confirmed the workaround.<br/><br/>\n",
    "            The workaround is a bit of a bore. It is to take advantage of \"WSL\", the Microsoft approach for running Linux on your PC. You'll need to carry out more setup instructions! But it's quick, and several students have confirmed that this works perfectly for them, then this lab and the Week 6 MCP labs work. Plus, WSL is actually a great way to build software on your Windows PC. You can also skip this final cell, but you will need to come back to this when we start Week 6.<br/>\n",
    "            The WSL Setup instructions are in the Setup folder, <a href=\"../setup/SETUP-WSL.md\">in the file called SETUP-WSL.md here</a>. I do hope this only holds you up briefly - you should be back up and running quickly. Oh the joys of working with bleeding-edge technology!<br/><br/>\n",
    "            With many thanks to student Kaushik R. for raising that this is needed here as well as week 6. Thanks Kaushik!\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from autogen_ext.tools.mcp import StdioServerParams, mcp_server_tools\n",
    "\n",
    "# Get the fetch tool from mcp-server-fetch.\n",
    "# And what this, this this is going to be running locally on our computers.\n",
    "# And because it's been written a certain way, that tool is just a tool we can use from Autogen.\n",
    "# So the key to MCP is that different people can write tools and as long as they write them a certain\n",
    "# way, you can just use them out of the box.\n",
    "# So MCP server fetch is an example of a tool that is open source and available, and that you can just\n",
    "# download and run it yourself.\n",
    "# And that is what this thing here does.\n",
    "# It runs it locally, and it is a tool which actually runs the playwright browser in a headless mode\n",
    "# and allows it to go and fetch web pages.\n",
    "# So it's doing something a bit similar to what we worked on with sidekick last week, but it's just doing\n",
    "# it not not in a way that brings up the browser, but just does it quietly behind the scenes, headless\n",
    "# as they call it.\n",
    "# And it will run that and then it will, it will, uh, get those tools and it will put those tools into\n",
    "# this thing called fetcher.\n",
    "# And we can just provide fetcher in as our tools.\n",
    "# That's it's, just as simple as that.\n",
    "# And so, uh, yeah, basically what we're doing here is that we're using a public online tool available\n",
    "# that runs Playwright Browser locally and uses that to scrape the web.\n",
    "# And we're making that tool available to our assistant.\n",
    "\n",
    "# And the reason it's cool is that we've just used a tool that someone else has written for, for running\n",
    "# playwrights in a headless way.\n",
    "# And we have just incorporated that tool because that tool uses this, this open standard, this standard\n",
    "# called MCP.\n",
    "# We're able to just drop that tool in and use it from within Autogen.\n",
    "# So just like we could use a Lang chain tool from within Autogen, we can use an MCP tool.\n",
    "# Anyone that's written a tool that conforms to the MCP standard.\n",
    "# And the cool thing about MCP is that it's such an open standard that anyone can write tools, and there\n",
    "# are websites where you can get access to lots and lots of these tools.\n",
    "# So it's like saying, we've got access to the Lang chain ecosystem only it's a whole lot more.=\n",
    "# It's this massive open source, public community ecosystem of tools.\n",
    "# Anyone that writes tools that conforms to the MCP standard, you can then access just like this, and\n",
    "# you can do it from within autogen in this way and immediately have access to any of them.\n",
    "\n",
    "fetch_mcp_server = StdioServerParams(command=\"uvx\", args=[\"mcp-server-fetch\"])\n",
    "fetcher = await mcp_server_tools(fetch_mcp_server)\n",
    "\n",
    "# Create an agent that can use the fetch tool.\n",
    "model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "agent = AssistantAgent(name=\"fetcher\", model_client=model_client, tools=fetcher, reflect_on_tool_use=True)  # type: ignore\n",
    "\n",
    "# Let the agent fetch the content of a URL and summarize it.\n",
    "result = await agent.run(task=\"Review edwarddonner.com and summarize what you learn. Reply in Markdown.\")\n",
    "display(Markdown(result.messages[-1].content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
