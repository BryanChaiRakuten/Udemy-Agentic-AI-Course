{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### And now - Week 3 Day 3\n",
    "\n",
    "## AutoGen Core\n",
    "\n",
    "Something a little different.\n",
    "\n",
    "This is agnostic to the underlying Agent framework\n",
    "\n",
    "You can use AutoGen AgentChat, or you can use something else; it's an Agent interaction framework.\n",
    "\n",
    "From that point of view, it's positioned similarly to LangGraph.\n",
    "\n",
    "### The fundamental principle\n",
    "\n",
    "Autogen Core decouples an agent's logic from how messages are delivered. (Decouples an Agent's logic from how messages are delivered)\n",
    "The framework provides a communication infrastructure, along with agent lifecycle, and the agents are responsible for their own work. (The framework handles creation and communucation)\n",
    "The Agents are responsible for their logic - that is not the remit of Autogen Core\n",
    "\n",
    "The communication infrastructure is called a Runtime.\n",
    "\n",
    "There are 2 types: **Standalone** and **Distributed**.\n",
    "\n",
    "One of them is called standalone, which basically essentially means it sort of runs on your box in\n",
    "a simple way. And the other distributed is that it runs in a way that could allow remote agents to interact with each\n",
    "other. So these are the two kinds, and you code them both a bit differently.\n",
    "\n",
    "Today we will use a standalone runtime: the **SingleThreadedAgentRuntime**, a local embedded agent runtime implementation.\n",
    "\n",
    "Tomorrow we'll briefly look at a Distributed runtime.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from autogen_core import AgentId, MessageContext, RoutedAgent, message_handler\n",
    "from autogen_core import SingleThreadedAgentRuntime\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "from autogen_agentchat.messages import TextMessage\n",
    "from autogen_ext.models.openai import OpenAIChatCompletionClient\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we define our Message object\n",
    "\n",
    "Whatever structure we want for messages in our Agent framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a simple one!\n",
    "# So the first thing we do is we define our own object which is going to be used for passing information\n",
    "# around the place, our own message object.\n",
    "# data class means it's a class that's not going to have any feel any, any methods. All it is , is something that holds data.\n",
    "# used to transport information between our agents.\n",
    "\n",
    "# And in a way, like doing this is sort of analogous to when we were in Landgraaf.\n",
    "# We started by defining a state.\n",
    "# Landgraaf is a state machine.\n",
    "# It's very focused on what is the state and making sure that that's something that you can move backwards\n",
    "# and forwards in.\n",
    "# And it's interesting that, you know, auto Jens auto gen core fundamental idea is all about messaging.\n",
    "# And so the thing that you start by defining is your message.\n",
    "\n",
    "@dataclass\n",
    "class Message:\n",
    "    content: str\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we define our Agent\n",
    "\n",
    "A subclass of RoutedAgent. which is the typical the thing that you that you have as your as your parent as your superclass.\n",
    "\n",
    "Every Agent has an **Agent ID** which has 2 components:  \n",
    "`agent.id.type` describes the kind of agent it is  \n",
    "`agent.id.key` gives it its unique identifier\n",
    "\n",
    "Any method with the `@message_handler` decorated will have the opportunity to receive messages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And it's important to bear in mind that the agent that we're about to create in Autogen core is different to the agent that we just created in Agent Chat.\n",
    "# Well, look, the agent that you define in Autogen core is just like a wrapper.\n",
    "# It's like saying this is a thing that can be messaged, it can be created, i can be managed, it can\n",
    "# be referred to and it can be messaged.\n",
    "# But what you do with this is your responsibility and you're going to have to delegate to something.\n",
    "# But this, this, this is like a management object, if you will, a management object which you're\n",
    "# going to use.\n",
    "# And it has amongst other things, it has an agent ID and every agent has a unique ID, and that ID has\n",
    "# two, two parts to it, a type and a key.\n",
    "# So every single agent has a type and a key.\n",
    "# And that combination of type and key is then unique and can uniquely identify it.\n",
    "\n",
    "# So this gives you a sort of good sense of the fabric of it, and that an agent class that you make is\n",
    "# not actually an LLM.\n",
    "# It's not an Autogen agent chat agent.\n",
    "# It's just this management holder object, something which has a type and a key.\n",
    "# So here is our first Autogen core agent.\n",
    "\n",
    "class SimpleAgent(RoutedAgent):\n",
    "    # Constructor\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(\"Simple\")\n",
    "\n",
    "    # Coroutine message handler\n",
    "    # Any method that you decorate with message handler means that potentially this is something that will\n",
    "    # receive messages and Autogen core handles.\n",
    "    # The fact that you'll be able to register this, and a runtime is going to manage this.\n",
    "    # And if someone sends a message to something with my name and my ID, then it's going to end up here.\n",
    "    # At least it's going to end up here.\n",
    "    # If what they dispatch is a message of this class.\n",
    "\n",
    "    # So this is a bit of a pro thing, but you can have multiple different classes and you can use that to\n",
    "    # be able to handle different types of message.\n",
    "    # Uh this may be more detail than you need, but you you could, for example, have a text message and\n",
    "    # an image message and have two separate handlers.\n",
    "    # And just by virtue of the different signatures, the different method signatures, Autogen core will\n",
    "    # automatically dispatch the right message to the right method, the right coroutine.\n",
    "\n",
    "    # Uh, and so this, this simple, uh, agent is going to return a, it's going to receive a message and\n",
    "    # it's going to return a message is now that I said it is a bit like this idea that in Landgraf we, uh,\n",
    "    # took in our nodes, took a state and returned a state.\n",
    "    # But this is just like a parallel thing.\n",
    "    # But it's all about messages.\n",
    "\n",
    "    @message_handler\n",
    "    async def on_my_message(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        # Message class defined above\n",
    "        # This eample isn't going to do anything with the message it receives, no LLM calls or anything like that.\n",
    "        return Message(content=f\"This is {self.id.type}-{self.id.key}. You said '{message.content}' and I disagree.\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OK let's create a Standalone runtime and register our agent type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to create a runtime, a single threaded agent runtime it's called, which is a standalone\n",
    "# runtime running on my computer, which as it says, will handle agents in a single threaded way.\n",
    "# And the first thing that we're going to do is register.\n",
    "# We call register on the agent itself to say, I want you to register yourself with this runtime.\n",
    "# Now, this isn't creating an agent.\n",
    "# This is just saying you are a type of agent.\n",
    "# And I want you to tell the runtime that you are a type of agent that can be spawned, you can be created,\n",
    "# and you are a type of agent of type simple agent that's going to be your type.\n",
    "# And this, this thing here, this is a function which can generate new versions of you.\n",
    "# It can instantiate you.\n",
    "# It is a factory.\n",
    "# And you pass that in as well.\n",
    "\n",
    "# We haven't actually yet built any of these.\n",
    "# We haven't instantiated one.\n",
    "# But the as a type of agent is now a known thing to our runtime.\n",
    "# Our runtime knows that there are such things as simple agents.\n",
    "runtime = SingleThreadedAgentRuntime()\n",
    "await SimpleAgent.register(runtime, \"simple_agent\", lambda: SimpleAgent())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alright! Let's start a runtime and send a message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is now running, its is a proper runtime\n",
    "runtime.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now we are going to uh, we're going to first of all create an agent ID object to identify an agent.\n",
    "# And we'll we'll do this properly.\n",
    "# We'll say agent ID equals that, that is an ID that would identify this.\n",
    "# And we are going to say that we want the default agent.\n",
    "# Uh, because, because that, that will then, uh, we'll make sure that we have an agent created,\n",
    "# and then we're going to send a message to, we're going to send a message to the, to the agent, which\n",
    "# is called simple agent, The default ID and we're going to send a message.\n",
    "# Well hi there to it.\n",
    "# And then we will print whatever comes back.\n",
    "\n",
    "# So it's simple agent is its type, which is the same one we registered and it default is the, uh,\n",
    "# the, the id uh, and it said, you said, well hi there.\n",
    "\n",
    "# What it's there to do is to handle the passing of messages around by looking things up based on their\n",
    "# type and their ID, and that's what it does.\n",
    "\n",
    "agent_id = AgentId(\"simple_agent\", \"default\")\n",
    "response = await runtime.send_message(Message(\"Well hi there!\"), agent_id)\n",
    "print(\">>>\", response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await runtime.stop()\n",
    "await runtime.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OK Now let's do something more interesting\n",
    "\n",
    "We'll use an AgentChat Assistant!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# And we could just call OpenAI directly using OpenAI dot create.\n",
    "# We could just call the Python client library.\n",
    "# But we can also use Autogen agent chat and we might as well since it's autogen week.\n",
    "\n",
    "class MyLLMAgent(RoutedAgent):\n",
    "    def __init__(self) -> None:\n",
    "        # init just just calls the superclass.\n",
    "        super().__init__(\"LLMAgent\")\n",
    "        model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\")\n",
    "        # And we're going to set a field called underscore delegate, which we're just sort of holding on to.\n",
    "        # This is this is the the underlying.\n",
    "        # This is what our agent object is going to delegate to when it actually needs some code, some, some\n",
    "        # an LLM to run.\n",
    "        # And we could have anything we want.\n",
    "        # If you remember, when you create an assistant agent, there's the name of the agent and there's the\n",
    "        # model client, and there it is.\n",
    "        # And as a result in underscore delegate underscore often used to show sort of like private secret, uh,\n",
    "        # variables that other people should not know about, like, like private in the, in the Java world.\n",
    "\n",
    "        self._delegate = AssistantAgent(\"LLMAgent\", model_client=model_client)\n",
    "\n",
    "    # Um, and that's really to sort of draw your attention to the fact that what matters is what type of\n",
    "    # object comes in the message field, because Autogen core will automatically route messages to agents\n",
    "    # that should receive them based on the the finding a handler that that looks for a message of that type.\n",
    "    # That's how it works.\n",
    "    # That's the clever routing that it does.\n",
    "    # And by the way, you can send a message direct to an agent, which is what we're doing right now.\n",
    "    # But it also has a whole kind of pub sub thing there where you can subscribe to topics and you can publish\n",
    "    # out messages to lots of agents that are all interested in one particular topic.\n",
    "    # So the whole process of dispatching messages to the right agent and then calling the right handler,\n",
    "    # that's the clever stuff.\n",
    "\n",
    "    # So anyways, handle my message type.\n",
    "    # Uh, as long as, uh, message is sent to this agent with the ID and and type, and that the object\n",
    "    # that's sent is of this type, it will arrive at this function.\n",
    "    # And I'm going to say that I received a message and print the content.\n",
    "    # Then I'm going to now now this is confusing.\n",
    "    # This object here text message looks subtly different to this object here.\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        print(f\"{self.id.type} received message: {message.content}\")\n",
    "        # This text message is text message from the, um, autogen.\n",
    "        text_message = TextMessage(content=message.content, source=\"user\")\n",
    "\n",
    "        # So we create the sort of text message that you need for Autogen agent chat passing in the content of\n",
    "        # our message and the source is user.\n",
    "        # And this is the onmessage that we call in agent Chat land with this text message and the cancellation\n",
    "        # token if you remember that thing.\n",
    "        # And then we get back the content.\n",
    "        response = await self._delegate.on_messages([text_message], ctx.cancellation_token)\n",
    "        \n",
    "        reply = response.chat_message.content\n",
    "        print(f\"{self.id.type} responded: {reply}\")\n",
    "        return Message(content=reply)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_core import SingleThreadedAgentRuntime\n",
    "# Let's create a new runtime, a single threaded agent runtime, standalone runtime.\n",
    "# We're going to register two agents the simple agent and the LM agent.\n",
    "\n",
    "runtime = SingleThreadedAgentRuntime()\n",
    "await SimpleAgent.register(runtime, \"simple_agent\", lambda: SimpleAgent())\n",
    "await MyLLMAgent.register(runtime, \"LLMAgent\", lambda: MyLLMAgent())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime.start()  # Start processing messages in the background.\n",
    "response = await runtime.send_message(Message(\"Hi there!\"), AgentId(\"LLMAgent\", \"default\"))\n",
    "print(\">>>\", response.content)\n",
    "response =  await runtime.send_message(Message(response.content), AgentId(\"simple_agent\", \"default\"))\n",
    "print(\">>>\", response.content)\n",
    "response = await runtime.send_message(Message(response.content), AgentId(\"LLMAgent\", \"default\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await runtime.stop()\n",
    "await runtime.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OK now let's show this at work - let's have 3 agents interact!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen_ext.models.ollama import OllamaChatCompletionClient\n",
    "\n",
    "# Agent 1\n",
    "# Subclass of RoutedAgent\n",
    "class Player1Agent(RoutedAgent):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name)\n",
    "        model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\", temperature=1.0)\n",
    "        self._delegate = AssistantAgent(name, model_client=model_client)\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        text_message = TextMessage(content=message.content, source=\"user\")\n",
    "        response = await self._delegate.on_messages([text_message], ctx.cancellation_token)\n",
    "        return Message(content=response.chat_message.content)\n",
    "\n",
    "# Agent 2 \n",
    "# Subclass of RoutedAgent\n",
    "class Player2Agent(RoutedAgent):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name)\n",
    "        # Use local LLaMA model via Ollama (Can be edited to other models same as above)\n",
    "        model_client = OllamaChatCompletionClient(model=\"llama3.2\", temperature=1.0)\n",
    "        self._delegate = AssistantAgent(name, model_client=model_client)\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        text_message = TextMessage(content=message.content, source=\"user\")\n",
    "        response = await self._delegate.on_messages([text_message], ctx.cancellation_token)\n",
    "        return Message(content=response.chat_message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JUDGE = \"You are judging a game of rock, paper, scissors. The players have made these choices:\\n\"\n",
    "\n",
    "# Agent 3\n",
    "# Subclass of RoutedAgent\n",
    "class RockPaperScissorsAgent(RoutedAgent):\n",
    "    def __init__(self, name: str) -> None:\n",
    "        super().__init__(name)\n",
    "        model_client = OpenAIChatCompletionClient(model=\"gpt-4o-mini\", temperature=1.0)\n",
    "        self._delegate = AssistantAgent(name, model_client=model_client)\n",
    "\n",
    "    @message_handler\n",
    "    async def handle_my_message_type(self, message: Message, ctx: MessageContext) -> Message:\n",
    "        # And it puts that together into a message.\n",
    "        # It looks up the default ID, the default, uh, key for player one.\n",
    "        # That type, and it looks up the default for player two, that type.\n",
    "        # And it's looking them up.\n",
    "        # And it then as part of handling its message, it dispatches off a message to these other agents.\n",
    "\n",
    "        instruction = \"You are playing rock, paper, scissors. Respond only with the one word, one of the following: rock, paper, or scissors.\"\n",
    "        message = Message(content=instruction)\n",
    "        inner_1 = AgentId(\"player1\", \"default\")\n",
    "        inner_2 = AgentId(\"player2\", \"default\")\n",
    "        response1 = await self.send_message(message, inner_1)\n",
    "        response2 = await self.send_message(message, inner_2)\n",
    "        result = f\"Player 1: {response1.content}\\nPlayer 2: {response2.content}\\n\"\n",
    "        judgement = f\"{JUDGE}{result}Who wins?\"\n",
    "        message = TextMessage(content=judgement, source=\"user\")\n",
    "        response = await self._delegate.on_messages([message], ctx.cancellation_token)\n",
    "        return Message(content=result + response.chat_message.content)\n",
    "\n",
    "\n",
    "# So just to summarize, we have a total of three agents that we've defined here.\n",
    "# Three of these like agent managers these agent wrappers, they're not they're not real agents.\n",
    "# They delegate to real agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And quite simply, we create a threaded agent a single threaded agent runtime, which is the the simple\n",
    "# kind of local runtime.=\n",
    "# We register player one, we register player two and give it the ability to instantiate players one and two.\n",
    "# And then we register the judge, the rock, paper, scissors, and and have everything ready to go.\n",
    "runtime = SingleThreadedAgentRuntime()\n",
    "await Player1Agent.register(runtime, \"player1\", lambda: Player1Agent(\"player1\"))\n",
    "await Player2Agent.register(runtime, \"player2\", lambda: Player2Agent(\"player2\"))\n",
    "await RockPaperScissorsAgent.register(runtime, \"rock_paper_scissors\", lambda: RockPaperScissorsAgent(\"rock_paper_scissors\"))\n",
    "runtime.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_id = AgentId(\"rock_paper_scissors\", \"default\")\n",
    "message = Message(content=\"go\")\n",
    "response = await runtime.send_message(message, agent_id)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await runtime.stop()\n",
    "await runtime.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
