{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome back to Python Notebooks!\n",
    "\n",
    "Didja miss me??\n",
    "\n",
    "### And welcome to Week 4, Day 2 - introducing LangGraph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "import gradio as gr\n",
    "from langgraph.graph import StateGraph\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some useful constants\n",
    "\n",
    "nouns = [\"Cabbages\", \"Unicorns\", \"Toasters\", \"Penguins\", \"Bananas\", \"Zombies\", \"Rainbows\", \"Eels\", \"Pickles\", \"Muffins\"]\n",
    "adjectives = [\"outrageous\", \"smelly\", \"pedantic\", \"existential\", \"moody\", \"sparkly\", \"untrustworthy\", \"sarcastic\", \"squishy\", \"haunted\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our favorite first step! Crew was doing this for us, by the way.\n",
    "load_dotenv(override=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shout(text: Annotated[str, \"something to be shouted\"]) -> str:\n",
    "    print(text.upper())\n",
    "    return text.upper()\n",
    "\n",
    "shout(\"hello\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A word about \"Annotated\"\n",
    "\n",
    "You probably know this; type hinting is a feature in Python that lets you specify the type of something:\n",
    "\n",
    "`my_favorite_things: List`\n",
    "\n",
    "But you may not know this:\n",
    "\n",
    "You can also use something called \"Annotated\" to add extra information that somebody else might find useful:\n",
    "\n",
    "`my_favorite_things: Annotated[List, \"these are a few of mine\"]`\n",
    "\n",
    "LangGraph needs us to use this feature when we define our State object.\n",
    "\n",
    "It wants us to tell it what function it should call to update the State with a new value.\n",
    "\n",
    "This function is called a **reducer**.\n",
    "\n",
    "LangGraph provides a default reducer called `add_messages` which takes care of the most common case.\n",
    "\n",
    "And that hopefully explains why the State looks like this.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Define the State object\n",
    "\n",
    "You can use any python object; but it's most common to use a TypedDict or a Pydantic BaseModel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add_messages\n",
    "# So that is it's like a function. And it's a function that you can, you can annotate with. If you want to say, hey this is the reducer I'd like you to use.\n",
    "# It is a list.\n",
    "# So it consists of a list of things of messages.\n",
    "# And we are going to then because we're annotating it, we can provide an annotation that's ignored by\n",
    "# Python, but it can be used by Landgraf.\n",
    "# And that annotation is where we get to specify the reducer, the function that will be called in order\n",
    "# to combine one state with another.\n",
    "\n",
    "# All it does is it assumes this is a list.\n",
    "# And if you return something with with an and items in the list.\n",
    "# It just combines it with everything else in the list before it concatenates these lists together.\n",
    "\n",
    "\n",
    "class State(BaseModel):\n",
    "    messages: Annotated[list, add_messages]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Start the Graph Builder with this State class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And that is just a matter of calling this thing called state graph.\n",
    "# Instantiating a state graph passing in state.\n",
    "# And one thing to to get your head around here is that what I'm passing in there.\n",
    "# The thing I've just highlighted. It's not an object I'm not instantiating.\n",
    "# I'm not I'm not creating a state and passing that in with with messages and so on. Now I'm passing in the class.\n",
    "# I'm passing in the type of thing that represents our state. That is what I'm using to create my state graph.\n",
    "# And this is beginning the graph building process. This is part of the five steps before we actually run our Agentic framework.\n",
    "graph_builder = StateGraph(State)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Create a Node\n",
    "\n",
    "A node can be any python function.\n",
    "\n",
    "The reducer that we set before gets automatically called to combine this response with previous responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes an old state and it returns a new state\n",
    "def our_first_node(old_state: State) -> State:\n",
    "\n",
    "    reply = f\"{random.choice(nouns)} are {random.choice(adjectives)}\"\n",
    "    messages = [{\"role\": \"assistant\", \"content\": reply}]\n",
    "\n",
    "    new_state = State(messages=messages)\n",
    "\n",
    "    return new_state\n",
    "\n",
    "# add it to the graph that is being build\n",
    "# (node_name, function_represent_the_node)\n",
    "graph_builder.add_node(\"first_node\", our_first_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Create Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Signify the beginning of our workflow and the end of it.\n",
    "# And so here what we say is we want an edge to take us from the start to our first node. And then we want another edge to go from the first node to the end.\n",
    "graph_builder.add_edge(START, \"first_node\")\n",
    "graph_builder.add_edge(\"first_node\", END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Compile the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the graph, our workflow\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's it! Showtime!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember gradio chat functions take the user's current input and the history of prior inputs, and it's\n",
    "# meant to respond with the next output.\n",
    "def chat(user_input: str, history):\n",
    "    message = {\"role\": \"user\", \"content\": user_input} # turn the message into a standard OpenAI format and put into a message\n",
    "    messages = [message]\n",
    "    state = State(messages=messages) # Create state object with that as the message\n",
    "    result = graph.invoke(state) # Invoke our graph, and this is the key langgraph word invoke, So you invoke a graph in land graph with the state in order to get the result. And that's what's going to execute our graph. And what will come out will be the result.\n",
    "    print(\"11111\")\n",
    "    print(result) # And we will print it and we will also return it. And that will come out of our chat function.\n",
    "    print(\"22222\")\n",
    "    print(result[\"messages\"])\n",
    "    print(\"33333\")\n",
    "    print(result[\"messages\"][-1])\n",
    "    print(\"44444\")\n",
    "    print(result[\"messages\"][1])\n",
    "    return result[\"messages\"][-1].content\n",
    "\n",
    "\n",
    "gr.ChatInterface(chat, type=\"messages\").launch()\n",
    "\n",
    "# Langgraph setup has nothing to do with LLMs specifically\n",
    "# The node is just a function, in this case a silly function, but it's a function there and it's taking\n",
    "# in a state and it's returning a state, and it doesn't need to have anything to do with llms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### But why did I show you that?\n",
    "\n",
    "To make the point that LangGraph is all about python functions - it doesn't need to involve LLMs!!\n",
    "\n",
    "Now we'll do the 5 steps again, but in 1 shot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define the State object\n",
    "class State(BaseModel):\n",
    "    messages: Annotated[list, add_messages]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Start the Graph Builder with this State class\n",
    "graph_builder = StateGraph(State)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Create a Node\n",
    "# So chat OpenAI is a construct from Lang Chain the sibling to Landgraf.\n",
    "# Uh and that's what we'll be using to connect with our LLM.\n",
    "# You can use any Llms.\n",
    "# You could directly call the LLM yourself. \n",
    "# You could also, uh, use maybe OpenAI agents SDK.\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "\n",
    "# takes old_state and return new_state\n",
    "def chatbot_node(old_state: State) -> State:\n",
    "    response = llm.invoke(old_state.messages) # takes the LLM and it invokes on that LLM, passing in masseges from old_state\n",
    "    # And then for the new state, it creates a new state object which contains within it as in its messages\n",
    "    # field, it contains the response.\n",
    "    # And we return the new state. And we add that node called chatbot, uh, into our graph builder.\n",
    "    new_state = State(messages=[response]) \n",
    "    return new_state\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create Edges\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph_builder.add_edge(\"chatbot\", END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Compile the Graph\n",
    "graph = graph_builder.compile()\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### That's it! And, let's do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And then we put it all together in a simple gradio chat function.\n",
    "# It takes an initial state, which is a state object set up with these messages like so.\n",
    "# We then call graph dot invoke to actually call our graph.\n",
    "# We print the result and we will also show the results back in Gradio.\n",
    "\n",
    "# But one thing that's worth noting is that if I continue this conversation every time we are invoking\n",
    "# this graph, and you will see what you have probably already suspected, which is that we're not actually\n",
    "# keeping track of any history here.\n",
    "\n",
    "# So because we've just got this simple graph that we were invoking each time, there's nothing particularly\n",
    "# interesting happening here.\n",
    "# And the state uh, is just, just contains that that uh, doesn't contain the history or anything.\n",
    "\n",
    "def chat(user_input: str, history):\n",
    "    initial_state = State(messages=[{\"role\": \"user\", \"content\": user_input}])\n",
    "    result = graph.invoke(initial_state)\n",
    "    print(result)\n",
    "    return result['messages'][-1].content\n",
    "\n",
    "\n",
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
